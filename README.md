# CelloAssignment-AvrahamKahana

# Monitoring
I myself did not play with monotoring infrastructure other than logging (timestamped Serilogs with daily rolling interval), but I had a bit exposure to Open Telemetry ideas/concepts in both Sepio and Startup Booster.
Adding Telemetry data to a project means enabling it to gather metrics, traces and logs in a standardized way that enables performing visual analysis on the collected data, throughout the period of time it is collected (timeline). (Prometheus, Grafana are open source exemples to support collecting/visualizing telemetry data. Cloud Providers also offer their own tools. Azure, for example, offers Application Insights SDK so code can report metrics, and Analytical cloud dashboards for visualizing the data).
It is also offers fundamental help when debugging/trobleshooting today's distributed workflows scattered throughout different several microservices, where one request passes through several other services till it completes. Tools such as Jaeger help in dissecting and visualizing the flow of such calls (basically the idea relies on the fact that all involved microservices APIs all share a unique identical id - called Correlation Id - by which one can in the end correlate logically related api calls together into a workflow).
In Startup Booster, we would use specific Kusto database functions that would query and list all web api calls sharing the same correlation id, and you would get a list, in chronlogical order, showing the status and result of each call, making it indispensable for troubleshooting problems.
As for metrics, the idea is generally to have discrete numerical values - say - at every Transaction Worker call to fetch all Parking Transactions - storing the amount of cpu time, memory usage, elapsed time for this operation. Such values are stored together with a timestamp, making a time series out of it. Then, visualization tools can display graphs showing the trends/offsets in the usage/consumed times, helping to identify bottlenecks, etc.

# Testing
As for testing, a rule of thumb is the (S) of the SOLID acronym, meaning striving to build neat components that perform a single specific task/responsibility - which makes Unit Testing it way simpler and leverages code reusability (once a component has a clear intent, it is easier to reuse it somewhere else).
Another rule of thumb for UTs is to benefit from Dependency Injection. Reusing a component in another one can be easily accomplished by injecting it into another - and injected code can be dummied/faked allowing unit tests to focus on testing stricly the functionality that is core to the component being tested. For instance, my PaymentController has a ParkingPaymentCalculator, which is the component that ultimately contains the logic for calculating parking cost. Although the flow requires that ParkingPaymentCalculator passes through PaymentController, by decoupling the two I can easily UT PaymentController and mock/fake everything else/non-PaymentController.
I am familiar with xUnit, FluentAssertions. Played briefly with the paid version of Telerik's JustMock, which gives the ability to mock objects without the need to implement an interface, for instance. At the end I still feel more comfortable with xUnit and FluentAssertions.
One thing that is tricky is to be able to scatter UTs across different UT projects/differnt test classes to improve performance of release pipelines that run UTs. We dealt with it a bit at Sepio Cyber.
As for Functional Tests, the farther I got was using WireMock to mock Http workflows.
